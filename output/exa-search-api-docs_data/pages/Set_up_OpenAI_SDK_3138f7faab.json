{
  "title": "Set up OpenAI' SDK",
  "content": "from openai import OpenAI\n\nopenai_api_key = \"YOUR_API_KEY_HERE\"\nopenai_client = OpenAI(api_key=openai_api_key)\nPython Python theme={null}\n\nquestions = [\n    \"How did bats evolve their wings?\",\n    \"How did Rome defend Italy from Hannibal?\",\n]\nPython Python theme={null}",
  "code_samples": [
    {
      "code": "Now, we just need some questions to answer!",
      "language": "unknown"
    },
    {
      "code": "While LLMs can answer some questions on their own, they have limitations:\n\n* LLMs don't have knowledge past when their training was stopped, so they can't know about recent events\n* If an LLM doesn't know the answer, it will often 'hallucinate' a correct-sounding response, and it can be difficult and inconvenient to distinguish these from correct answers\n* Because of the opaque manner of generation and the problems mentioned above, it is difficult to trust an LLM's responses when accuracy is [important](https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/?sh=27194eb67c7f)\n\nRobust retrieval helps solve all of these issues by providing quality sources of ground truth for the LLM (and their human users) to leverage and cite. Let's use Exa to get some information to answer our questions:",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#set-up-openai'-sdk",
  "links": []
}
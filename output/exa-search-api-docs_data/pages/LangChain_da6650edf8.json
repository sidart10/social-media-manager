{
  "title": "LangChain",
  "content": "Source: https://docs.exa.ai/reference/langchain\n\nHow to use Exa's integration with LangChain to perform RAG.\n\nLangChain is a framework for building applications that combine LLMs with data, APIs and other tools. In this guide, we'll go over how to use Exa's LangChain integration to perform RAG with the following steps:\n\n1. Set up Exa's LangChain integration and use Exa to retrieve relevant content\n2. Connect this content to a toolchain that uses OpenAI's LLM for generation\n\n<Info> See a YouTube tutorial of a very similar setup by the LangChain team [here](https://www.youtube.com/watch?v=dA1cHGACXCo). </Info>\n\n<Info> See the full reference from LangChain [here](https://python.langchain.com/docs/integrations/providers/exa%5Fsearch/). </Info>\n\n<Steps>\n  <Step title=\"Pre-requisites and installation\">\n    Install the core OpenAI and Exa LangChain libraries\n\n<Note> Ensure API keys are initialized properly. For LangChain libraries, the environment variable names are `OPENAI_API_KEY` and `EXA_API_KEY` for OpenAI and Exa keys respectively. </Note>\n\n<Card title=\"Get your Exa API key\" icon=\"key\" horizontal href=\"https://dashboard.exa.ai/api-keys\" />\n  </Step>\n\n<Step title=\"Use Exa Search to power a LangChain Tool\">\n    Set up a Retriever tool using `ExaSearchRetriever`. This is a retriever that connects to Exa Search to find relevant documents via semantic search. First import the relevant libraries and instantiate the ExaSearchRetriever.\n\n<Step title=\"Create a prompt template (optional)\">\n    We use a LangChain [PromptTemplate](https://python.langchain.com/v0.1/docs/modules/model%5Fio/prompts/quick%5Fstart/#prompttemplate) to define a template of placeholder to parse out URLs and Highlights from the Exa retriever.\n\n<Step title=\"Parse the URL and content from Exa results\">\n    We use a [Runnable Lambda](https://api.python.langchain.com/en/latest/runnables/langchain%5Fcore.runnables.base.RunnableLambda.html) to parse out the URL and Highlights attributes from the Exa Search results then pass this to the prompt template above\n\n<Step title=\"Join Exa results and content for retrieval\">\n    Complete the retrieval chain by stitching together the Exa retriever, the parser and a short lambda function - this is crucial for passing the result as a single string as context for the LLM in the next step.\n\n<Step title=\"Set up the rest of the toolchain including OpenAI for generation\">\n    In this step, we define the system prompt with Query and Context template inputs to be grabbed from the user and Exa Search respectively. First, once again import the relevant libraries and components from LangChains libraries\n\nThen we define a generation prompt - the prompt template that is used with context from Exa to perform RAG.\n\nWe set the generation [LLM to OpenAI](https://python.langchain.com/v0.1/docs/integrations/chat/openai/), then connect everything with a [RunnableParallel](https://python.langchain.com/v0.1/docs/expression%5Flanguage/primitives/parallel/) parallel connection. The generation prompt, containing the query and context, is then passed to the LLM and [parsed for better output representation](https://api.python.langchain.com/en/latest/output%5Fparsers/langchain%5Fcore.output%5Fparsers.string.StrOutputParser.html).\n\n<Step title=\"Running the full RAG toolchain\">\n    Let's [invoke](https://python.langchain.com/v0.1/docs/expression%5Flanguage/interface/#invoke) the chain:\n\nAnd have a look at the output (newlines parsed):\n\n<Step title=\"Optionally, stream the output of the chain\">\n    Optionally, you may\n\nOutputs, in a stream - [click here](https://python.langchain.com/v0.1/docs/expression%5Flanguage/streaming/) to learn more about the .stream method and other options, including handling of chunks and how to think about further parsing outputs:\n\nAs you can see, the output generation is enriched with the context of our Exa Search query result!\n  </Step>\n</Steps>",
  "code_samples": [
    {
      "code": "<Note> Ensure API keys are initialized properly. For LangChain libraries, the environment variable names are `OPENAI_API_KEY` and `EXA_API_KEY` for OpenAI and Exa keys respectively. </Note>\n\n    <Card title=\"Get your Exa API key\" icon=\"key\" horizontal href=\"https://dashboard.exa.ai/api-keys\" />\n  </Step>\n\n  <Step title=\"Use Exa Search to power a LangChain Tool\">\n    Set up a Retriever tool using `ExaSearchRetriever`. This is a retriever that connects to Exa Search to find relevant documents via semantic search. First import the relevant libraries and instantiate the ExaSearchRetriever.",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Create a prompt template (optional)\">\n    We use a LangChain [PromptTemplate](https://python.langchain.com/v0.1/docs/modules/model%5Fio/prompts/quick%5Fstart/#prompttemplate) to define a template of placeholder to parse out URLs and Highlights from the Exa retriever.",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Parse the URL and content from Exa results\">\n    We use a [Runnable Lambda](https://api.python.langchain.com/en/latest/runnables/langchain%5Fcore.runnables.base.RunnableLambda.html) to parse out the URL and Highlights attributes from the Exa Search results then pass this to the prompt template above",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Join Exa results and content for retrieval\">\n    Complete the retrieval chain by stitching together the Exa retriever, the parser and a short lambda function - this is crucial for passing the result as a single string as context for the LLM in the next step.",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Set up the rest of the toolchain including OpenAI for generation\">\n    In this step, we define the system prompt with Query and Context template inputs to be grabbed from the user and Exa Search respectively. First, once again import the relevant libraries and components from LangChains libraries",
      "language": "unknown"
    },
    {
      "code": "Then we define a generation prompt - the prompt template that is used with context from Exa to perform RAG.",
      "language": "unknown"
    },
    {
      "code": "We set the generation [LLM to OpenAI](https://python.langchain.com/v0.1/docs/integrations/chat/openai/), then connect everything with a [RunnableParallel](https://python.langchain.com/v0.1/docs/expression%5Flanguage/primitives/parallel/) parallel connection. The generation prompt, containing the query and context, is then passed to the LLM and [parsed for better output representation](https://api.python.langchain.com/en/latest/output%5Fparsers/langchain%5Fcore.output%5Fparsers.string.StrOutputParser.html).",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Running the full RAG toolchain\">\n    Let's [invoke](https://python.langchain.com/v0.1/docs/expression%5Flanguage/interface/#invoke) the chain:",
      "language": "unknown"
    },
    {
      "code": "And have a look at the output (newlines parsed):",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Optionally, stream the output of the chain\">\n    Optionally, you may",
      "language": "unknown"
    },
    {
      "code": "Outputs, in a stream - [click here](https://python.langchain.com/v0.1/docs/expression%5Flanguage/streaming/) to learn more about the .stream method and other options, including handling of chunks and how to think about further parsing outputs:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Get Started",
      "id": "get-started"
    }
  ],
  "url": "llms-txt#langchain",
  "links": []
}
{
  "title": "Hallucination Detector",
  "content": "Source: https://docs.exa.ai/examples/demo-hallucination-detector\n\nA live demo that detects hallucinations in content using Exa's search.\n\n<div>\n  <a href=\"https://demo.exa.ai/hallucination-detector\" target=\"_blank\" rel=\"noopener noreferrer\">\n    <button class=\"api-button\">\n      \\> try the app\n    </button>\n  </a>\n</div>\n\nWe built a live hallucination detector that uses Exa to verify LLM-generated content. When you input text, the app breaks it into individual claims, searches for evidence to verify each one, and returns relevant sources with a verification confidence score.\n\nA claim is a single, verifiable statement that can be proven true or false - like \"The Eiffel Tower is in Paris\" or \"It was built in 1822.\"\n\n<Card title=\"Click here to try it out.\" href=\"https://demo.exa.ai/hallucination-detector\" img=\"https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=75ac7324f228fb3d0d19f18603998228\" data-og-width=\"1832\" width=\"1832\" data-og-height=\"1170\" height=\"1170\" data-path=\"images/Screenshot 2024-11-19 at 3.19.48â€¯PM.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?w=280&fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=efd378652d3d4b95bc8ce2eeb30d2c7d 280w, https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?w=560&fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=c3413cedc97f7d2d99428cbb248782da 560w, https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?w=840&fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=2f854f02938ef3992bbf8a28097d50fa 840w, https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?w=1100&fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=f27814fd501730881b12e9b5f98e7342 1100w, https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?w=1650&fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=a6fbfa924159fbddfd541a18a8bbd6f5 1650w, https://mintcdn.com/exa-52/tmzyKnsgpKLGddKC/images/Screenshot%202024-11-19%20at%203.19.48%E2%80%AFPM.png?w=2500&fit=max&auto=format&n=tmzyKnsgpKLGddKC&q=85&s=f57f2087ea2cbb27c371c21117b25703 2500w\" />\n\nThis document explains the functions behind the three steps of the fact-checker:\n\n1. The LLM extracts verifiable claims from your text\n2. Exa searches for relevant sources for each claim\n3. The LLM evaluates each claim against its sources, returning whether or not its true, along with a confidence score.\n\n<Info>See the full [step-by-step guide](/examples/identifying-hallucinations-with-exa) and [github repo](https://github.com/exa-labs/exa-hallucination-detector) if you'd like to recreate. </Info>\n\n## Function breakdown\n\n<Steps>\n  <Step title=\"Extracting claims\">\n    The `extract_claims` function uses an LLM (Anthropic's, in this case) to identify distinct, verifiable statements from your inputted text, returning these claims as a JSON array of strings.\n\n<Warning>For simpilicity, we did not include a try/catch block in the code below. However, if you are building your own hallucination detector, you should include one that catches any errors in the LLM parsing and uses a regex method that treats each sentence (text between capital letter and end punctuation) as a claim.</Warning>\n\n<Step title=\"Searching for evidence\">\n    The `exa_search` function uses Exa search to find evidence for each extracted claim. For every claim, it retrieves the 5 most relevant sources, formats them with their URLs and content (`text`), passing them to the next function for verification.\n\n<Step title=\"Verifying claims\">\n    The `verify_claim` function checks each claim against the sources from `exa_search`. It uses an LLM to determine if the sources support or refute the claim and returns a decision with a confidence score. If no sources are found, it returns \"insufficient information\".\n\nUsing LLMs to extract claims and verify them against Exa search sources is a simple way to detect hallucinations in content. If you'd like to recreate it, the full documentation for the script is [here](/examples/identifying-hallucinations-with-exa) and the github repo is [here](https://github.com/exa-labs/exa-hallucination-detector).",
  "code_samples": [
    {
      "code": "</Step>\n\n  <Step title=\"Searching for evidence\">\n    The `exa_search` function uses Exa search to find evidence for each extracted claim. For every claim, it retrieves the 5 most relevant sources, formats them with their URLs and content (`text`), passing them to the next function for verification.",
      "language": "unknown"
    },
    {
      "code": "</Step>\n\n  <Step title=\"Verifying claims\">\n    The `verify_claim` function checks each claim against the sources from `exa_search`. It uses an LLM to determine if the sources support or refute the claim and returns a decision with a confidence score. If no sources are found, it returns \"insufficient information\".",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Function breakdown",
      "id": "function-breakdown"
    }
  ],
  "url": "llms-txt#hallucination-detector",
  "links": []
}
